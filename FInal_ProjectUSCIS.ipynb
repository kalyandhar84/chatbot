{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chatbot Kiti\n",
    "Here I am planning to build an information-based chatbot which will dive deep into US Citizenship and Immigration services (USCIS) policy manual and able to response user queries. This USCIS Policy Guide includes the official policies of USCIS and supports immigration-officers for making decisions. The idea is to have non-immigrant policy as source of this research. This research will contain 137 legal documents which is summarized under USCIS Volume 2(https://www.uscis.gov/policy-manual/volume-2) User should be able to ask questions related to any non-immigration related visas and get non-legal advice from this bot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Zj9h92wUDPN"
   },
   "source": [
    "**Importing the required libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "4BmyocFbb0MJ"
   },
   "outputs": [],
   "source": [
    "import numpy as np  #For numerical computation in python\n",
    "import nltk         #For natural language processing\n",
    "import string       #process strings in python\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "85QE5FDSUKqU"
   },
   "source": [
    "**Importing and reading the corpus**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "jouIkYEkb9Pk"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kdhar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\kdhar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "f=open('data_USCIS.txt','r',errors = 'ignore')\n",
    "raw_doc=f.read()\n",
    "raw_doc=raw_doc.lower() #Converts text to lowercase\n",
    "nltk.download('punkt') #Using the Punkt tokenizer. Other tokenizers inlcude tweepy, RegEx etc\n",
    "nltk.download('wordnet') #Using the WordNet dictionary\n",
    "sent_tokens = nltk.sent_tokenize(raw_doc) #Converts doc to list of sentences \n",
    "word_tokens = nltk.word_tokenize(raw_doc) #Converts doc to list of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pmXgGkVeUSUb"
   },
   "source": [
    "**sentance tokens**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Swu4WRVncPR8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['policy manual\\nthe uscis policy manual is the agencyâ€™s centralized online repository for uscisâ€™ immigration policies.',\n",
       " 'the uscis policy manual will ultimately replace the adjudicatorâ€™s field manual (afm), the uscis immigration policy memoranda site, and other policy repositories.',\n",
       " 'about the policy manual\\nthe uscis policy manual is the agencyâ€™s centralized online repository for uscisâ€™ immigration policies.',\n",
       " 'the policy manual is replacing the adjudicatorâ€™s field manual (afm), the uscis immigration policy memoranda site, and other uscis policy repositories.',\n",
       " 'the policy manual contains separate volumes pertaining to different areas of immigration benefits administered by the agency, such as citizenship and naturalization, adjustment of status, and nonimmigrants.',\n",
       " 'the content is organized into different volumes, parts, and chapters.',\n",
       " 'the policy manual provides transparency of immigration policies and furthers consistency, quality, and efficiency consistent with the uscis mission.',\n",
       " 'the policy manual provides all the latest policy updates; an expanded table of contents; keyword search function; and links to the immigration and nationality act and code of federal regulations, as well as public use forms.',\n",
       " 'the policy manual contains tables and charts to facilitate understanding of complex topics.',\n",
       " 'the policy manual also contains all historical policy updates.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokens[:10]  #Printing first Ten sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences in document: 960\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Total sentences in document:', len(sent_tokens ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chatbot will be based on information based. All policy data will be used as a text document. As we are working on policy document So, there are no missing text entries. This bot will work on nonstructural data. I am planning to perform several data normalization steps to normalize corps. The main question with text data is that it is all in unstructured text format (strings). However, the Machine learning algorithms can understand numerical feature vector in order to perform the task. I will be using he NLTK data package includes a pre-trained Punkt tokenizer for English. Here are few Key component That I will perform during My project as Removing Noise, Removing the Stop words, Perform Stemming operation, Perform Lemmatization operation, Perform NER, Perform POS, Perform Multiple Stem Operation. I am planning to use CountVectorizer from Sklearn-featureextraction to create Vocabulary of words from corpus. After that I am planning to perform the Term Frequency-Inverse Document Frequency (TF-IDF) vectors. TF-IDF will give us matrix. After that I will perform cosine similarity to compute a numeric measure that indicates the similarity between the two words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step involves word tokenization, Removing ASCII values, Removing tags of any kind, Part-of-speech tagging, and Lemmatization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gtkzd0KhUWJT"
   },
   "source": [
    "**word tokens**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "hcwrvmWicaLc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['policy',\n",
       " 'manual',\n",
       " 'the',\n",
       " 'uscis',\n",
       " 'policy',\n",
       " 'manual',\n",
       " 'is',\n",
       " 'the',\n",
       " 'agencyâ€™s',\n",
       " 'centralized']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokens[:10]  #Print first 10 words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bvvYcZZ9UbVD"
   },
   "source": [
    "**Text preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "YbZllVqBcc78"
   },
   "outputs": [],
   "source": [
    "lemmer = nltk.stem.WordNetLemmatizer()\n",
    "#WordNet is a semantically-oriented dictionary of English included in NLTK library.\n",
    "def LemTokens(tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tLX8WBE4UgOr"
   },
   "source": [
    "**Defining the greeting function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "dLOqphibchJM"
   },
   "outputs": [],
   "source": [
    "GREET_INPUTS = (\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\",\"hey\")   #sup is Millenial shortform for what's up?\n",
    "GREET_RESPONSES = [\"hi\", \"hey\", \"Hi, How are you?\", \"hi there\", \"hello\", \"I am glad! You are talking to me\"]\n",
    "\n",
    "def greet(sentence): \n",
    "    for word in sentence.split():\n",
    "        if word.lower() in GREET_INPUTS:\n",
    "            return random.choice(GREET_RESPONSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qJhFmyRCUm4j"
   },
   "source": [
    "**Response generation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "eo7Kv52HcjG0"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer  #Term frequency and inverse document frequency(for rare words)\n",
    "from sklearn.metrics.pairwise import cosine_similarity       #It gives normalized vectors to the machine for it to understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "JEHZesw3cnNM"
   },
   "outputs": [],
   "source": [
    "def response(user_response):\n",
    "  robo1_response=''\n",
    "  TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
    "  tfidf = TfidfVec.fit_transform(sent_tokens)\n",
    "  vals = cosine_similarity(tfidf[-1], tfidf)\n",
    "  idx=vals.argsort()[0][-2]\n",
    "  flat = vals.flatten()\n",
    "  flat.sort()\n",
    "  req_tfidf = flat[-2]\n",
    "  if(req_tfidf==0):\n",
    "    robo1_response=robo1_response+\"I am sorry! I don't understand you\"\n",
    "    return robo1_response\n",
    "  else:\n",
    "    robo1_response = robo1_response+sent_tokens[idx]\n",
    "    return robo1_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Q-iY_o1Utas"
   },
   "source": [
    "**Defining conversation start/end protocols**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "wxzENVDgdNGd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOT: My name is Kiti. Let's have a conversation! Also, if you want to exit any time, just type Bye!\n",
      "hi\n",
      "BOT: hello\n",
      "Hello Kiti\n",
      "BOT: hi\n",
      "Can I get Student Visa\n",
      "BOT: the nonimmigrant vocational student (m-1) visa category includes students in established vocational or other recognized nonacademic programs but excludes language training programs.\n",
      "How about H1B visa\n",
      "BOT: [4] in 2018, congress further extended the guam and cnmi h-2b and h-1b visa cap exemptions from 2019 to 2029.\n",
      "Can you tell me about dependent visa\n",
      "BOT: however, the dependents are not authorized to work in the united states while in the foreign information media representative dependent status.\n",
      "can dependent get work authorization?\n",
      "BOT: however, the dependents are not authorized to work in the united states while in the foreign information media representative dependent status.\n",
      "bye\n",
      "BOT: Goodbye! Take care <3 \n"
     ]
    }
   ],
   "source": [
    "flag=True\n",
    "print(\"BOT: My name is Kiti. Let's have a conversation! Also, if you want to exit any time, just type Bye!\")\n",
    "while(flag==True):\n",
    "    user_response = input()\n",
    "    user_response=user_response.lower()\n",
    "    if(user_response!='bye'):\n",
    "        if(user_response=='thanks' or user_response=='thank you' ):\n",
    "            flag=False\n",
    "            print(\"BOT: You are welcome..\")\n",
    "        else:\n",
    "            if(greet(user_response)!=None):\n",
    "                print(\"BOT: \"+greet(user_response))\n",
    "            else:\n",
    "                sent_tokens.append(user_response)\n",
    "                word_tokens=word_tokens+nltk.word_tokenize(user_response)\n",
    "                final_words=list(set(word_tokens))\n",
    "                print(\"BOT: \",end=\"\")\n",
    "                print(response(user_response))\n",
    "                sent_tokens.remove(user_response)\n",
    "    else:\n",
    "        flag=False\n",
    "        print(\"BOT: Goodbye! Take care <3 \")  #<3 is for heart shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Building a chatbot in Python",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
